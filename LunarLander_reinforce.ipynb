{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gym[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch import nn as nn\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "from utils import test_policy_network, seed_everything, plot_stats, plot_action_probs\n",
    "from parallel_env import ParallelEnv, ParallelWrapper\n",
    "import statistics as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and preprocess the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve number of states and number of actions\n",
    "\n",
    "dims = env.observation_space.shape[0] \n",
    "actions = env.action_space.n "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    seed_everything(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_fns = [lambda: create_env('LunarLander-v2') for _ in range(num_envs)] \n",
    "\n",
    "# use the function ParallelEnv to create the parallelized environment\n",
    "parallel_env = ParallelEnv(env_fns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the environment to work with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessEnv(ParallelWrapper):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        ParallelWrapper.__init__(self, env)\n",
    "    \n",
    "    def reset(self):\n",
    "        state = self.venv.reset()\n",
    "        return torch.from_numpy(state).float()\n",
    "    \n",
    "    def step_async(self, actions):\n",
    "        actions = actions.squeeze().numpy()\n",
    "        self.venv.step_async(actions)\n",
    "     \n",
    "    def step_wait(self):\n",
    "        next_state, reward, done, info = self.venv.step_wait()\n",
    "        next_state = torch.from_numpy(next_state).float()\n",
    "        reward = torch.tensor(reward).unsqueeze(1).float()\n",
    "        done = torch.tensor(done).unsqueeze(1)\n",
    "        return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_env = PreprocessEnv(parallel_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the policy $\\pi(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the case considered, the policy is adjusted. Therefore, only few example of the used policies are left for conciseness purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_C1 = nn.Sequential(\n",
    "    nn.Linear(dims, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, actions),\n",
    "    nn.Softmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_A3 = nn.Sequential(\n",
    "    nn.Linear(dims, 256),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(256, actions),\n",
    "    nn.Softmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_A4 = nn.Sequential(\n",
    "    nn.Linear(dims, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, actions),\n",
    "    nn.Softmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_D3 = nn.Sequential(\n",
    "    nn.Linear(dims,64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64,32),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(32, actions),\n",
    "    nn.Softmax(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, episodes, alpha=1e-4, gamma=0.99, beta = 0.01):\n",
    "    \n",
    "    # create the object that will update the parameters of the NN. The AdamW class is an improved version of the \n",
    "    #     stochastic gradient descent update rule. \n",
    "    optim = AdamW(policy.parameters(), lr=alpha)\n",
    "    \n",
    "    # declare the dictionary inside which the statistics of the execution of the algorithm are stored\n",
    "    stats = {'PG Loss': [], 'Returns': []}\n",
    "    \n",
    "    # repeat the main loop for every episode, wrapping it with the tqdm function to follow on the screen the execution of the loop\n",
    "    for episode in tqdm(range(1, episodes + 1)):\n",
    "        \n",
    "        # set the parallel environment to receive the initial state of every individual environment\n",
    "        state = parallel_env.reset()\n",
    "        \n",
    "        # boolean column vector that tells whether each of the individual environments has finished the episode or not\n",
    "        done_b = torch.zeros((num_envs, 1), dtype=torch.bool)\n",
    "        \n",
    "        # create a list to store the state transitions\n",
    "        transitions = []\n",
    "        \n",
    "        # used to keep track of the returns of each of the individual episodes\n",
    "        ep_return = torch.zeros((num_envs, 1))\n",
    "\n",
    "        # inner loop to generate the trajectory of experience. The loop goes on until all the episodes are finished\n",
    "        while not done_b.all():\n",
    "            \n",
    "            # the policy selects an action for every individual environment\n",
    "            action = policy(state).multinomial(1).detach()\n",
    "            \n",
    "            # execute the action in the environment and observe the outcome\n",
    "            next_state, reward, done, _ = parallel_env.step(action)\n",
    "            \n",
    "            # store the transition in the list of transitions (if the environment is finished, its reward shouldn't be updated.\n",
    "            #    That's why it's multiplied for ~done_b)\n",
    "            transitions.append([state, action, ~done_b * reward])\n",
    "            \n",
    "            # update the values\n",
    "            ep_return += reward\n",
    "            done_b |= done\n",
    "            state = next_state\n",
    "        \n",
    "        # initialize the return obtained for each parallel environment as a column vector of zeros\n",
    "        G = torch.zeros((num_envs, 1))\n",
    "        \n",
    "        # iterate over each moment of time in reverse order\n",
    "        for t, (state_t, action_t, reward_t) in reversed(list(enumerate(transitions))):\n",
    "            \n",
    "            # update the value of the return\n",
    "            G = reward_t + gamma * G\n",
    "            \n",
    "            # obtain the vector of probabilities for the state of each environment at a certain moment in time\n",
    "            #    and compute its log. \"+ 1e-6\" is added to avoid issues with the log in case of probabilities equal to 0.\n",
    "            probs_t = policy(state_t)\n",
    "            log_probs_t = torch.log(probs_t + 1e-6)\n",
    "            \n",
    "            # choose only the probability of the action acutally taken at time t\n",
    "            action_log_prob_t = log_probs_t.gather(1, action_t)\n",
    "            \n",
    "            # compute the entropy of each vector of probabilities at time t\n",
    "            entropy_t = - torch.sum(probs_t * log_probs_t, dim=-1, keepdim=True)\n",
    "            \n",
    "            # compute the estimate of the policy performance. The minus sign comes from the fact that the AdamW optimizer\n",
    "            #    can only perform gradient descent, but in the REINFORCE algorithm gradient ascent is performed.\n",
    "            gamma_t = gamma ** t\n",
    "            pg_loss_t = - gamma_t * action_log_prob_t * G\n",
    "            \n",
    "            # subtract the entropy scaled by beta and compute the mean over every environment that\n",
    "            #   the agent is facing in parallel\n",
    "            total_loss_t = (pg_loss_t - beta * entropy_t).mean()\n",
    "            \n",
    "            # wipe the gradients of the NN\n",
    "            policy.zero_grad()\n",
    "            \n",
    "            # use the backpropagation algorithm to compute the gradient of the loss function with respect to\n",
    "            #    each of the NN parameters\n",
    "            total_loss_t.backward()\n",
    "            \n",
    "            # update the NN parameters\n",
    "            optim.step()\n",
    "        \n",
    "        # update the dictionary with the statistics\n",
    "        stats['PG Loss'].append(total_loss_t.item())\n",
    "        stats['Returns'].append(ep_return.mean().item())\n",
    "        \n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the filtering function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_values(values, window=10):\n",
    "    weight = np.repeat(1.0, window)/window\n",
    "    smas = np.convolve(values,weight,'valid')\n",
    "    return filtered_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the reiforce algorithm with policy A3 for 10000 episodes\n",
    "\n",
    "A3 = reinforce(policy_A3, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the returns and the filtered returns\n",
    "\n",
    "returns_A3 = A3[\"Returns\"]\n",
    "filtered_A3 = filter_values(returns_A3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(returns_A3)\n",
    "ax.plot(filtered_A3)\n",
    "plt.axhline(y = 195,color = 'g')\n",
    "ax.set_ylabel('Returns')\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_title('Policy A3')\n",
    "p = stats.mean(returns_A3[1000:-1])\n",
    "plt.axhline(y = p, color = 'r')\n",
    "plt.legend(['Returns','Filtered Returns','Target Return','Average from 1000th episode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1 = reinforce(policy_C1, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_C1 = C1[\"Returns\"]\n",
    "filtered_C1 = filter_values(returns_C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ax.plot(returns_C1)\n",
    "ax.plot(filtered_C1)\n",
    "plt.axhline(y = 195,color = 'g')\n",
    "ax.set_ylabel('Returns')\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_title('Policy C1')\n",
    "p = stats.mean(returns_C1[1000:-1])\n",
    "plt.axhline(y = p, color = 'r')\n",
    "plt.legend(['Returns','Filtered Returns','Target Return','Average from 1000th episode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy A4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A4 = reinforce(policy_A4, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_A4 = A4[\"Returns\"]\n",
    "filtered_A4 = filter_values(returns_A4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ax.plot(returns_A4)\n",
    "ax.plot(filtered_A4)\n",
    "plt.axhline(y = 195,color = 'g')\n",
    "ax.set_ylabel('Returns')\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_title('Policy A4')\n",
    "p = stats.mean(returns_A4[1000:-1])\n",
    "plt.axhline(y = p, color = 'r')\n",
    "plt.legend(['Returns','Filtered Returns','Target Return','Average from 1000th episode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy D3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D3 = reinforce(policy_D3, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_D3 = D3[\"Returns\"]\n",
    "filtered_D3 = filter_values(returns_D3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ax.plot(returns_D3)\n",
    "ax.plot(filtered_D3)\n",
    "plt.axhline(y = 195,color = 'g')\n",
    "ax.set_ylabel('Returns')\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_title('Policy D3')\n",
    "p = stats.mean(returns_D3[1000:-1])\n",
    "plt.axhline(y = p, color = 'r')\n",
    "plt.legend(['Returns','Filtered Returns','Target Return','Average from 1000th episode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select in the next cell the policy that needs to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the name of the policy that needs to be tested\n",
    "policy = policy_A4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy_network(env, policy, episodes=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
